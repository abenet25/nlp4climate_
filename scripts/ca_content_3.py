import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
import pandas as pd
import pathlib
import spacy
from pathlib import Path
from wordcloud import WordCloud
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.exceptions import NotFittedError
from sklearn.utils.validation import check_is_fitted
from NLP_functions_3 import ( 
    generate_wordcloud_from_csv, 

    generate_topic_wordcloud,
    generate_topic_bar_chart,
    plot_word_vectors
)

# üìÇ Base path = wo main.py wird durchgef√ºhrt
BASE_PATH = Path(__file__).resolve().parent.parent
DATA_PATH = BASE_PATH / "data"
MODELS_PATH = BASE_PATH / "models"

for file in ["vectorizer.pkl", "logistic_regression.pkl", "naive_bayes.pkl"]:
    path = MODELS_PATH / file
    st.write(f"‚úÖ {file}: {path.exists()}")


# Klassifikator: Vectorizer und Modelle
@st.cache_resource
def load_classification_models():
    with open(MODELS_PATH / "vectorizer.pkl", "rb") as f1, \
         open(MODELS_PATH / "logistic_regression.pkl", "rb") as f2, \
         open(MODELS_PATH / "naive_bayes.pkl", "rb") as f3:
        return pickle.load(f1), pickle.load(f2), pickle.load(f3)
    



def show_ca_content():
    # Titel
    st.markdown("<h3 style='margin-bottom: 0;'>üåç Esperan√ßa de vida i canvi clim√†tic amb PLN</h3>", unsafe_allow_html=True)
    st.markdown("<h4 style='margin-top: 0;'>N√∫vols de mots, visualitzaci√≥ de temes, classificaci√≥ de frases</h4>", unsafe_allow_html=True)

    # Navigation
    st.sidebar.title("üß≠ Navegaci√≥")
    option = st.sidebar.radio(
        "Tria una secci√≥:",
        ("üìò Descripci√≥ del projecte", 
         "üì≤ Dades i metodologia", 
         "üóÇÔ∏è Visualitzaci√≥ de temes", 
         "üß† Classificaci√≥ de frases", 
         "üöÄ Possibles millores",
         "üìã Sobre el projecte")
    )

    if option == "üìò Descripci√≥ del projecte":
        show_project_description()
    elif option == "üì≤ Dades i metodologia":
        show_data_methodology()
    elif option == "üóÇÔ∏è Visualitzaci√≥ de temes":
        show_topic_visualization()
    elif option == "üß† Classificaci√≥ de frases":
        show_sentence_classification()
    elif option == "üöÄ Possibles millores":
        st.title("üöÄ Possibles millores")
        st.markdown("""
        ‚ÜóÔ∏è **Ampliar el corpus institucional i el period√≠stic**  
        Aix√≠ com el corpus acad√®mic s'ha pogut fornir amb √®xit a trav√©s d'una API (Semantic Scholar) i s'ha recopilat un bon nombre d'articles, 
        no s'ha aconseguit el mateix amb els altres dos corpus. 
        L'objectiu √©s poder extreure igualment de manera autom√†tica un gran nombre de textos institucionals i period√≠stics, 
        mitjan√ßant una API o *Web Scraping*.     
        Aix√≤ augmentaria i equilibraria la base de dades i milloraria la classificaci√≥. \n    
        üëå**Augmentar la sensibilitat del classificador**         
        Actualment, el model nom√©s intenta classificar cada frase dins d‚Äôuna de les tres fonts predefinides (acad√®mica, 
        institucional o period√≠stica). Una millora futura seria incorporar un mecanisme que detecti si la frase introdu√Øda 
        pertany al domini del corpus global. Aix√≤ permetria oferir **una predicci√≥ sobre el grau de versemblan√ßa de la frase dins del context analitzat** 
        i alertar quan el seu contingut s‚Äôallunya significativament dels textos del corpus original.\n
        üß± **Millorar la modularitat de l'estructura interna del projecte**         
        La llargada de l‚Äôscript principal (de_content.py / ca_content.py) es pot reduir i gestionar millor si es separen 
        les funcions en m√≤duls independents.    
        Aix√≠ mateix, l‚Äôestructura de les dades (models, corpus i diccionaris) pot ser reorganitzada 
        per afavorir una millor mantenibilitat i escalabilitat.      
        """)
    elif option == "üìã Sobre el projecte":
        st.title("üìã Sobre el projecte")
        st.markdown("""
        **Autora:** Ariadna Benet  
        **Projecte realitzat dins el [Data Science Bootcamp ‚Äì Data Science Institute by Fabian Rappert](https://www.data-science-institute.de)**  
        **Any:** 2025  
        
        üìä Dataset de punt de partida: [Countries Life Expectancy (Kaggle)](https://www.kaggle.com/datasets/amirhosseinmirzaie/countries-life-expectancy)  
        üìö Llibreries: Streamlit, Gensim, scikit-learn, wordcloud, Matplotlib, NumPy, Pandas, Seaborn

        üîß Aquest projecte empra t√®cniques de processament del llenguatge natural (PLN), com LDA, Word2Vec i classificadors supervisats.
        """)


def show_project_description():
    st.markdown("<h2 style='margin-bottom: 0;'>üìòDescripci√≥ del projecte</h2>", unsafe_allow_html=True)
    st.markdown("""
    Benvinguts al nostre projecte de processament del llenguatge natural (PLN)!  
    Aquest treball √©s part del projecte final **"Esperan√ßa de vida: An√†lisis amb intel¬∑lig√®ncia de negoci (*Business Intelligence*), 
    aprenentatge autom√†tic (*Machine Learning*) i processament del llenguatge natural"**, dut a terme en el marc del **Data Science Bootcamp** 
    (formaci√≥ en ci√®ncia de dades) al [Data Science Institute by Fabian Rappert](https://www.data-science-institute.de).

    üìä **Font de dades**  
    El projecte parteix del conjunt de dades üëâ [Countries Life Expectancy](https://www.kaggle.com/datasets/amirhosseinmirzaie/countries-life-expectancy) *(Kaggle)*.
    Les an√†lisis amb intel¬∑lig√®ncia de negoci i amb aprenentatge autom√†tic es basen en aquest conjunt de dades.
    Es pregunten quines s√≥n les variables m√©s determinants per a l'esperan√ßa de vida en tots els continents. I fan previsions sobre com poden evolucionar les xifres 
    en funci√≥ de com canvi√Øn els diversos factors.  

    üåé **Pregunta de recerca**  
    Un cop examinades les dades sobre l'esperan√ßa de vida, **ens preguntem si el canvi clim√†tic afecta o pot afectar l‚Äôesperan√ßa de vida**. 
    Quins efectes poden tenir sobre la llargada de la vida fen√≤mens globals com ara l'augment de les temperatures o la contaminaci√≥? S√≥n els mateixos en tots els pa√Øsos, rics o pobres? 
    I els efectes m√©s locals del canvi clim√†tic, com ara les cat√†strofes naturals, han deixat evid√®ncies ja sobre l'esperan√ßa de vida? Els pa√Øsos desenvolupats es podran veure m√©s afectats 
    pel canvi clim√†tic que per certes malalties, per exemple? 
    
    üî≠ **Enfocament**    
    Atesa la dificultat de vincular directament conjunts de dades num√®riques dels dos √†mbits, optem per un enfocament alternatiu: 
    analitzem textos sobre les dues tem√†tiques mitjan√ßant t√®cniques d‚Äôaprenentatge autom√†tic, a fi de respondre les nostres preguntes de recerca.        
     
    üéØ **Objectiu**            
    Ens proposem, doncs, **enlla√ßar el tema de l'esperan√ßa de vida amb el del canvi clim√†tic** tot analitzant **textos en angl√®s** sobre els dos temes, provinents de **tres fonts diferents**:
    
    - üë©‚Äçüè´ Publicacions acad√®miques  
    - üèõÔ∏è Informes institucionals  
    - üì∞ Articles period√≠stics  
           
    üóÇÔ∏è Examinem els temes m√©s freq√ºents en textos procedents de les tres fonts.       
    üß† Entrenem un classificador de frases que prediu de quina d‚Äôaquestes tres fonts prov√© una frase determinada.
                
    ‚öôÔ∏è **Metodologia**     
    Mitjan√ßant l'**an√†lisi de temes** (*Topic Modeling*), **representacions sem√†ntiques de paraules** (*Word Embeddings*) i **m√®todes de classificaci√≥** (amb *Multinomial Naive Bayes* i regressi√≥ log√≠stica), 
    obtenim una visi√≥ sobre la representaci√≥ ling√º√≠stica d'aquestes tem√†tiques globals.   
    A manca de dades num√®riques que enllacin els dos √†mbits, **convertim textos i paraules en vectors i xifres**.
                
    """, unsafe_allow_html=True)

def show_data_methodology():
    st.markdown("<h2 style='margin-bottom: 0;'>üì≤ Dades i metodologia</h2>", unsafe_allow_html=True)
    # Daten
    st.subheader("üîç Dades recollides")
    st.markdown("""
    Els corpus s'han recollit i netejat mitjan√ßant API i manualment. En total analitzem:

    - 432 resums acad√®mics (API de Semantic Scholar)
    - 22 informes institucionals (OMS, ONU, Banc Mundial)
    - 29 articles period√≠stics (BBC, Euronews, Reuters, The Conversation, entre d'altres)
                
    Mots clau per a la cerca: "life expectancy" AND "climate change"; "health" AND "climate change"
    
    Objectiu d'optimitzaci√≥: 
    Per aconseguir un millor equilibri entre les tres fonts, 
    caldria ampliar els corpus institucional i period√≠stic ‚Äîidealment mitjan√ßant *scraping* automatitzat amb APIs 
    (vegeu üöÄ Possibles millores).            
 
    En total treballem amb m√©s de **154.000 paraules**.
    """)
 
    # Daten
    labels = ["Acad√®mia", "Institucions", "Mitjans de comunicaci√≥"]
    sizes = [96909, 29716, 27720]
    colors = ["#66b3ff", "#99ff99", "#ffcc99"]
    total = sum(sizes)

    # Bargraph
    fig, ax = plt.subplots(figsize=(4, 1.8))  

    bars = ax.barh(labels, sizes, color=colors)
    ax.set_xlabel("Nombre de mots", fontsize=8)
    ax.set_title("Distribuci√≥ dels corpus segons la font", fontsize=8)

    # Werte neben die Bars
    for bar, count in zip(bars, sizes):
        percent = count / total * 100
        text = f"{count:,} ({percent:.1f}%)".replace(",", ".") 
        ax.text(count + total * 0.01, bar.get_y() + bar.get_height()/2,
                text, va='center', fontsize=7)

    # Format
    ax.tick_params(axis='both', labelsize=7)
    ax.set_xlim(0, max(sizes)*1.15)
    fig.tight_layout(pad=1)

    st.pyplot(fig)

    # Prozent und absolute Werte
    def make_autopct(values):
        def my_autopct(pct):
            total = sum(values)
            val = int(round(pct * total / 100.0))
            return f"{pct:.1f}%\n({val:,} mots)".replace(',', 'X').replace('.', ',').replace('X', '.')
        return my_autopct



    # Methoden
    st.subheader("‚öôÔ∏è M√®todes i procediment")
    st.markdown("""
    Per al desenvolupament del projecte, s'han dut a terme aquests passos:

    1. **Preprocessament**: tot el text a min√∫scules, eliminaci√≥ de car√†cters especials i separacions de l√≠nia.
    A m√©s, espec√≠ficament per a cada part: 
    
        
    | M√≤dul              | Tokenitzaci√≥      | Unitat d'an√†lisi | *Bigrams* | *Stopwords*                              | Etiqueta |
    |--------------------|-------------------|------------------|-----------|------------------------------------------|----------|
    | **N√∫vols de mots**     | ‚úÖ        | Mot             | ‚úÖ      | Angl√®s, mots clau, espec√≠fics del corpus | ‚ùå     |
    | **LDA**            | ‚úÖ                | Mot             | ‚úÖ      | Angl√®s,  mots clau, espec√≠fics del corpus | ‚ùå     |
    | **Word2Vec**       | ‚úÖ                | Mot             | ‚úÖ      | Angl√®s, espec√≠fics del corpus                | ‚ùå     |
    | **Classificaci√≥** | ‚úÖ                | Frase             | ‚ùå      | cap                                     | ‚úÖ     |

    """)           
    with st.expander("‚úÇÔ∏è Qu√® √©s la *tokenitzaci√≥*?"):
        st.write("""**La tokenitzaci√≥** √©s el primer pas en l‚Äôan√†lisi automatitzada del llenguatge natural (PLN).   
        \nConsisteix a dividir un text en unitats m√©s petites anomenades *tokens*, com ara paraules o frases.   
        \nAquests tokens serveixen com a base per a l‚Äôan√†lisi posterior, com la detecci√≥ de temes o la classificaci√≥.""")

    with st.expander("üëØ‚Äç‚ôÄÔ∏è Qu√® √©s un *bigram*?"):
        st.write("""Un **bigram** √©s una parella de paraules consecutives que sovint apareixen juntes en un text
        i que formen una unitat de significat.
        \n**Exemple:** la parella *"air pollution"* √©s un bigram perqu√® aquestes dues paraules sovint apareixen juntes
        i signifiquen *contaminaci√≥ de l‚Äôaire*.""")

    with st.expander("ü´∏ Qu√® s√≥n els *stopwords*?"):
        st.write("""Els **stopwords** (o **mots buits**) s√≥n paraules molt freq√ºents com ara *i*, *el*, *√©s* o *de*,
        que habitualment no aporten gaire informaci√≥ i es descarten en les an√†lisis.
        \nüîπ **Angl√®s:** Per a cada llengua s'apliquen llistes de mots buits espec√≠fiques. Es tracta sobretot de 
                 mots funcionals, com ara en angl√®s, per exemple, *the*, *is*, *and*, etc.  
        üîπ **Mots clau:** Termes massa dominants com ara *health* o *climate change* tamb√© s'eliminen, per evitar biaixos.  
        üîπ **Espec√≠fics del corpus:** s'exclouen paraules molt freq√ºents dins d‚Äôun corpus concret per√≤ amb poc valor anal√≠tic ‚Äîcom *study*, *analysis*, o *conclusions* en textos acad√®mics.
        """)
    
    st.markdown("""
    
    2. **Aprenentatge no supervisat** (√©s a dir, sense etiquetes):  
    - **N√∫vols de mots** per a cada corpus  
    - **Modelatge de temes amb LDA** per visualitzar temes
    - **Word2Vec** aplicat a tot el corpus
    3. **Aprenentatge supervisat** (√©s a dir, amb etiquetes):  
    - Classificaci√≥ de frases: amb els models **Multinomial Naive Bayes** i **Regressi√≥ log√≠stica**
    - L'usuari pot introduir una frase i rebr√† la font m√©s probable (corpus acad√®mic, institucional o period√≠stic)
    """)

def show_topic_visualization():
    st.title("üóÇÔ∏è Visualitzaci√≥ de temes")
    st.markdown("""
    Aqu√≠ es mostren els temes m√©s rellevants de cada corpus mitjan√ßant: 
    
    - üî† **N√∫vols de mots**  
    - üìä **Modelatge de temes amb LDA** 
    - üìê **Word2Vec**
    """)

    # üìÇ Directori de dades
    # BASE_PATH = pathlib.Path().resolve()
    # DATA_PATH = BASE_PATH / "data"


    # üóÇÔ∏è Fitxers per a cada corpus
    corpus_paths = {
        "Wissenschaft": str(DATA_PATH / "corpus_academia.csv"),
        "Institutionen": str(DATA_PATH / "corpus_institutions.csv"),
        "Medien": str(DATA_PATH / "corpus_media.csv"),
}

    st.subheader("üî† **N√∫vols de mots**")
 
    # Auswahl des Korpuses

    corpus_labels = {
        "Wissenschaft": "Acad√®mic",
        "Institutionen": "Institucional",
        "Medien": "Period√≠stic"
    }

    corpus_choice_label = st.selectbox(
        "Tria un corpus:",
        list(corpus_labels.values()),
        key="lda_corpus"
    )

    label_to_key = {v: k for k, v in corpus_labels.items()}
    corpus_choice = label_to_key[corpus_choice_label]

    # Path zum .csv
    csv_path = corpus_paths[corpus_choice]

    # Word Cloud generieren und zeigen
    with st.expander(f"N√∫vol de mots del corpus: {corpus_choice_label}", expanded=True):
        wc = generate_wordcloud_from_csv(csv_path)
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis("off")
        st.pyplot(fig) 



    df = pd.read_csv(csv_path)  # DataFrame amb BoW
    lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
    lda_model.fit(df)

    vocab = df.columns  # noms de les paraules

    # Funktion zur Generierung der Thema-Bezeichnungen
    def get_topic_labels(lda_model, vocab, topn=1):
        labels = []
        for i, topic in enumerate(lda_model.components_):
            top_indices = topic.argsort()[::-1][:topn]
            main_word = vocab[top_indices[0]]
            labels.append((i, f"Tema {i+1} ‚Äì {main_word}"))
        return labels
    
    # Themenliste aufbauen
    topic_labels = get_topic_labels(lda_model, vocab)
    label_to_id = {label: idx for idx, label in topic_labels}

    st.subheader("üìä **Modelatge de temes amb LDA**" )

    # Thema ausw√§hlen
    topic_label = st.selectbox("Tria un tema", list(label_to_id.keys()))
    topic_id = label_to_id[topic_label]

    # Visualitza
    wc_topic = generate_topic_wordcloud(lda_model, topic_id, vocab)
    fig_bar = generate_topic_bar_chart(lda_model, topic_id, vocab)
    """
    col1, col2 = st.columns(2)
    with col1:
        wc_topic = generate_topic_wordcloud(lda_model, topic_id)
        fig_wc, ax_wc = plt.subplots(figsize=(12, 6))
        ax_wc.imshow(wc_topic, interpolation='bilinear')
        ax_wc.axis('off')
        st.pyplot(fig_wc)
    with col2:
        fig_bar = generate_topic_bar_chart(lda_model, topic_id)
        st.pyplot(fig_bar)
    """    
st.markdown("""
    Aquests temes han estat extrets del corpus mitjan√ßant un model LDA.  
    Cada tema est√† format per paraules que sovint apareixen juntes en un mateix context.  
    De vegades, una mateixa paraula pot apar√®ixer en diversos temes d'un grup. Aix√≤ passa perqu√® √©s una paraula molt freq√ºent 
    en el corpus i juga un paper rellevant en diferents √†mbits tem√†tics.  
    LDA: *Latent Dirichlet Allocation*.
    """)
   
# Canviar la descripci√≥ a SpaCy
st.subheader("üìê **Word2Vec:** Descobrint relacions sem√†ntiques dels mots")

st.markdown("""
    **Word2Vec** √©s un model que representa les paraules com a vectors num√®rics, basant-se en els contextos sem√†ntics del mot dins del corpus textual.  
    Aquests vectors solen tenir una dimensionalitat alta (p. ex., 100), per√≤ es redueixen a dues dimensions 
    mitjan√ßant una **PCA** (an√†lisi de components principals) per tal de poder-los visualitzar millor.  
    Aquesta representaci√≥ permet identificar relacions entre termes i mostrar-les de manera clara en un espai bidimensional.

    üìç En el seg√ºent gr√†fic es pot veure una projecci√≥ PCA d‚Äôaquests vectors.  
    Com m√©s a prop es troben dos punts, m√©s semblants han estat interpretats pel model.
    """)


    # Modell laden

@st.cache_resource
def load_spacy_model():
    return spacy.load("en_core_web_md")

    nlp = load_spacy_model()

    # üåê PCA-Visualisierung

    input_words = st.text_input(
        "Mots que vols visualitzar (separats per espai):",
        "climate_change life_expectancy health air_pollution policy"
    )

    words_list = [w.strip().lower() for w in input_words.split()]
    words_found = [w for w in words_list if nlp(w)[0].has_vector]
    words_not_found = [w for w in words_list if not nlp(w)[0].has_vector]

    # Wenn kein Word im Modell
    if not words_found:
        st.error("‚ö†Ô∏è Cap dels mots es troben dins el model.")
    else:
        # Einige W√∂rter sind nicht im Modell
        if words_not_found:
            st.warning(f"‚ö†Ô∏è Els seg√ºents mots **no** es troben dins el model i han estat ignorats: {', '.join(words_not_found)}")
        
        # Visualisierung f√ºr die gefundene W√∂rter
        fig = plot_spacy_vectors(nlp, words_found)
        if fig:
            st.pyplot(fig)

def show_sentence_classification():
    st.title("üß† Classificaci√≥ de frases")
    st.markdown("""Escriu una frase en angl√®s sobre l‚Äôesperan√ßa de vida i el canvi clim√†tic, i el nostre model et donar√† 
                la predicci√≥ de la font original m√©s probable ‚Äî√©s a dir, si la frase √©s m√©s probable de ser trobada 
                en un text acad√®mic, institucional o period√≠stic.    
                Prova per exemple amb: "Climate change affects life expectancy"; despr√©s afegeix "in Europe".
                """)

    vectorizer, model_lr, model_nb = load_classification_models()

    try:
        check_is_fitted(vectorizer, "idf_")
    except NotFittedError:
        st.error("‚ùå El vectoritzador no ha estat entrenat. Sisplau, comprova l'arxiu 'vectorizer.pkl'.")
        st.stop()
        
    # Benutzereingabe
    user_input = st.text_input("‚úèÔ∏è Escriu una frase:")

    label_translation = {
        "academic": "acad√®mic",
        "institutional": "institucional",
        "media": "period√≠stic"
    }

    if user_input:
        # In Vektoren umwandeln und vorhersagen
        X_input = vectorizer.transform([user_input])
        pred = model_lr.predict(X_input)[0]
        probas = model_lr.predict_proba(X_input)[0]
        classes = model_lr.classes_

        pred_label = label_translation.get(pred, pred)
        st.success(f"Font m√©s probable: **corpus {pred_label}**")

        # Wahrscheinlichkeiten anzeigen
        st.markdown("### üé≤ Probabilitats:")
        for label, prob in zip(classes, probas):
            translated_label = label_translation.get(label, label)
            st.write(f"‚Ä¢ Corpus **{translated_label}**: {prob * 100:.1f}%".replace('.', ','))
        
        # Barchart der Wahrscheinlichkeit

        import matplotlib.ticker as mtick

        translated_classes = [label_translation.get(label, label) for label in classes]
        proba_df = pd.DataFrame({"Classe": translated_classes, "Probabilitat": probas})
        proba_df = proba_df.sort_values("Probabilitat", ascending=True)

        fig, ax = plt.subplots(figsize=(3, 1.2))
        ax.barh(proba_df["Classe"], proba_df["Probabilitat"], color="skyblue")
        ax.set_xlim(0, 1)
        ax.set_xlabel("Probabilitat", fontsize=4)
        ax.set_title("Distribuci√≥ de la classificaci√≥", fontsize = 5)
        ax.tick_params(axis='both', labelsize=5)
        ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f"{x:.1f}".replace('.', ',')))
        fig.tight_layout(pad=1)
        st.pyplot(fig)

        # Vergleich Multinomial Naive Bayes - Logistic Regression
        st.markdown("## ü§ñ Elecci√≥ del model per a la classificaci√≥")
            
        st.markdown("""En primer lloc, es va aplicar el model **Multinomial Naive Bayes**.  
                        Per√≤ com que com que la **precisi√≥** era relativament baixa (**67,56%**),  
                        i sobretot perqu√® molts enunciats van ser classificats com a *acad√®mics* (vegeu la *matriu de confusi√≥*),  
                        es va considerar que aquest model estava fortament influ√Øt per la **sobrerrepresentaci√≥ de frases acad√®miques**.
                        """)
        st.markdown("""Per aix√≤ a continuaci√≥ es va provar la **regressi√≥ log√≠stica**.  
                        Aquest model no nom√©s ofereix una **precisi√≥** m√©s alta (**75,35%**),  
                        sin√≥ tamb√© uns resultats **molt m√©s equilibrats** pel que fa a *Precision*,  
                        *Recall* i *F1-score*, especialment en les classes *institucional* i *period√≠stica*.  
                        Finalment, doncs, es va optar per la **regressi√≥ log√≠stica** com a model per a la classificaci√≥.
                        """)
        
        st.markdown("### Matriu de confusi√≥:")

        with open(MODELS_PATH / "naive_bayes.pkl", "rb") as f:
            nb_model = pickle.load(f)

        with open(MODELS_PATH / "logistic_regression_tr.pkl", "rb") as f:
            lr_model = pickle.load(f)

        with open(MODELS_PATH / "X_test_tfidf.pkl", "rb") as f1, open(MODELS_PATH /"y_test.pkl", "rb") as f2:
            X_test_tfidf = pickle.load(f1)
            y_test = pickle.load(f2)

        nb_preds = nb_model.predict(X_test_tfidf)
        lr_preds = lr_model.predict(X_test_tfidf)   
           
        def plot_confusion_matrix(y_true, y_pred, labels, title):
            cm = confusion_matrix(y_true, y_pred, labels=labels)
            fig, ax = plt.subplots(figsize=(2, 2))
            heatmap = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        square= True,
                        xticklabels=labels, yticklabels=labels, 
                        annot_kws={"size": 5}, ax=ax, 
                        cbar_kws={"shrink": 0.5})
            cbar = heatmap.collections[0].colorbar
            cbar.ax.tick_params(labelsize=5)
            
            ax.set_title(title, fontsize=6)
            ax.set_xlabel("Predicted", fontsize=5)
            ax.set_ylabel("Actual", fontsize=5)
            ax.tick_params(axis='both', labelsize=4)
            heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=0, fontsize=3)
            heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0, fontsize=3)

            fig.tight_layout(pad=1)
            st.pyplot(fig)
        

        col1, col2 = st.columns(2)

        with col1:
            plot_confusion_matrix(y_test, nb_preds, labels=classes, title="Naive Bayes")

        with col2:
            plot_confusion_matrix(y_test, lr_preds, labels=classes, title="Regressi√≥ log√≠stica")   


        st.markdown("### Informes de classificaci√≥:")

        
        def classification_report_to_df(report_str):
            lines = report_str.strip().split("\n")
            data = []
            for line in lines[2:-3]:
                parts = line.strip().split()
                if len(parts) == 5:
                    label = parts[0]
                    precision, recall, f1, support = map(float, parts[1:])
                    data.append((label, precision, recall, f1, int(support)))
            return pd.DataFrame(data, columns=["Label", "Precision", "Recall", "F1-Score", "Support"])

        def style_report_table(df):
            return (
                df.style
                .format({"Precision": "{:.2f}", "Recall": "{:.2f}", "F1-Score": "{:.2f}"})
                .set_properties(subset=["Precision", "Recall", "F1-Score", "Support"], **{"text-align": "right"})
                .set_table_styles([
                    {"selector": "th", "props": [("text-align", "right")]},  # Alineaci√≥ de cap√ßaleres
                ])
            )

        report_nb_str = classification_report(y_test, nb_preds, target_names=model_nb.classes_)
        report_lr_str = classification_report(y_test, lr_preds, target_names=lr_model.classes_)

        report_nb_df = classification_report_to_df(report_nb_str)
        report_lr_df = classification_report_to_df(report_lr_str)

        # Calcular l'accuracy manualment
        accuracy_nb = accuracy_score(y_test, nb_preds)
        accuracy_lr = accuracy_score(y_test, lr_preds)

        st.markdown("#### Multinomial Naive Bayes")
        st.markdown(f"‚û°Ô∏è **Precisi√≥**: {accuracy_nb:.2%}")
        st.dataframe(style_report_table(report_nb_df))

        st.markdown("#### Logistic Regression üèÖ ")
        st.markdown(f"‚û°Ô∏è **Precisi√≥**: {accuracy_lr:.2%}")
        st.dataframe(style_report_table(report_lr_df))
